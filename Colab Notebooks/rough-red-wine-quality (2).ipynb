{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":114,"outputs":[{"output_type":"stream","text":"/kaggle/input/red-wine-quality-cortez-et-al-2009/winequality-red.csv\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import time\nimport seaborn as sns\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import LabelEncoder \nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.filterwarnings('ignore')\n\n \ndata = pd.read_csv('../input/red-wine-quality-cortez-et-al-2009/winequality-red.csv')\n\n\ndef get_score(model , X_train , X_test, Y_train , Y_test):\n\tmodel.fit(X_train ,Y_train)\n\treturn model.score(X_test ,Y_test)\n\ndef fill_missing(data):\n    for col in data.columns:\n        data[col] = data[col].fillna(data[col].mean())\n \n  \n \nfill_missing(data)\n\n\nX = data.drop(['quality'] , axis = 'columns')\nY = data['quality']\n\nX.head()\n         \n\n \n","execution_count":115,"outputs":[{"output_type":"execute_result","execution_count":115,"data":{"text/plain":"   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n0                 11.0                  34.0   0.9978  3.51       0.56   \n1                 25.0                  67.0   0.9968  3.20       0.68   \n2                 15.0                  54.0   0.9970  3.26       0.65   \n3                 17.0                  60.0   0.9980  3.16       0.58   \n4                 11.0                  34.0   0.9978  3.51       0.56   \n\n   alcohol  \n0      9.4  \n1      9.8  \n2      9.8  \n3      9.8  \n4      9.4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>density</th>\n      <th>pH</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.9968</td>\n      <td>3.20</td>\n      <td>0.68</td>\n      <td>9.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.9970</td>\n      <td>3.26</td>\n      <td>0.65</td>\n      <td>9.8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.9980</td>\n      <td>3.16</td>\n      <td>0.58</td>\n      <td>9.8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.9978</td>\n      <td>3.51</td>\n      <td>0.56</td>\n      <td>9.4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size=0.20, random_state=0) \n\n\n#classification models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\n\nmodelNames=['DecisionTreeClassifier', 'RandomForestClassifier',\n            'KNeighborsClassifier', 'GaussianNB', \n            'LogisticRegression', 'svm', 'MLPClassifier']\nmodel = [DecisionTreeClassifier() , \n        RandomForestClassifier(),\n       KNeighborsClassifier() ,  \n       GaussianNB(),\n       LogisticRegression(),\n       svm.LinearSVC(),\n        MLPClassifier(random_state=1, max_iter=300)\n        ]\n\n#evaluation\nfrom sklearn.metrics import accuracy_score , confusion_matrix, classification_report\n \n\n","execution_count":116,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('normal = ' ,get_score(model[0] ,X_train, X_test, Y_train, Y_test))\nfrom sklearn.feature_selection import SelectKBest, chi2 ,  RFE  \nfrom sklearn.model_selection import  cross_val_score\n# sel_f = SelectKBest(chi2, k = 7).fit(X,Y)\n# sel_bool = sel_f.get_support() \n \n\n# for i in range(1,len(X.columns)+1):\n#     sel_f = SelectKBest(chi2, k = i).fit(X,Y)\n#     sel_bool = sel_f.get_support() \n#     cols= X.columns[sel_bool]\n#     print(i , get_score(model[0] , X_train[cols] ,X_test[cols] , Y_train , Y_test))\n     \n# print()\n\n# for i in range(1,len(X.columns)+1):\n#     sel_f = SelectKBest(chi2, k = i).fit(X,Y)\n#     sel_bool = sel_f.get_support() \n#     cols= X.columns[sel_bool]\n#     print(i , get_score(model[1] , X_train[cols] ,X_test[cols] , Y_train , Y_test))\n\n# #we get that we must take 9 best features\n\n \n","execution_count":117,"outputs":[{"output_type":"stream","text":"normal =  0.65\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sel_f = SelectKBest(chi2, k = 9).fit(X,Y)\nsel_bool = sel_f.get_support()\nX = X[X.columns[sel_bool]]\nX.head()\n","execution_count":118,"outputs":[{"output_type":"execute_result","execution_count":118,"data":{"text/plain":"   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n0            7.4              0.70         0.00             1.9      0.076   \n1            7.8              0.88         0.00             2.6      0.098   \n2            7.8              0.76         0.04             2.3      0.092   \n3           11.2              0.28         0.56             1.9      0.075   \n4            7.4              0.70         0.00             1.9      0.076   \n\n   free sulfur dioxide  total sulfur dioxide  sulphates  alcohol  \n0                 11.0                  34.0       0.56      9.4  \n1                 25.0                  67.0       0.68      9.8  \n2                 15.0                  54.0       0.65      9.8  \n3                 17.0                  60.0       0.58      9.8  \n4                 11.0                  34.0       0.56      9.4  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fixed acidity</th>\n      <th>volatile acidity</th>\n      <th>citric acid</th>\n      <th>residual sugar</th>\n      <th>chlorides</th>\n      <th>free sulfur dioxide</th>\n      <th>total sulfur dioxide</th>\n      <th>sulphates</th>\n      <th>alcohol</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.56</td>\n      <td>9.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>7.8</td>\n      <td>0.88</td>\n      <td>0.00</td>\n      <td>2.6</td>\n      <td>0.098</td>\n      <td>25.0</td>\n      <td>67.0</td>\n      <td>0.68</td>\n      <td>9.8</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>7.8</td>\n      <td>0.76</td>\n      <td>0.04</td>\n      <td>2.3</td>\n      <td>0.092</td>\n      <td>15.0</td>\n      <td>54.0</td>\n      <td>0.65</td>\n      <td>9.8</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11.2</td>\n      <td>0.28</td>\n      <td>0.56</td>\n      <td>1.9</td>\n      <td>0.075</td>\n      <td>17.0</td>\n      <td>60.0</td>\n      <td>0.58</td>\n      <td>9.8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7.4</td>\n      <td>0.70</td>\n      <td>0.00</td>\n      <td>1.9</td>\n      <td>0.076</td>\n      <td>11.0</td>\n      <td>34.0</td>\n      <td>0.56</td>\n      <td>9.4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in model:\n    print(i ,cross_val_score(i , X,Y ,scoring = 'accuracy' ,cv =  4))\n    \n#we find that best model is random forest classifier\n\n    ","execution_count":119,"outputs":[{"output_type":"stream","text":"DecisionTreeClassifier() [0.475     0.4425    0.46      0.4962406]\nRandomForestClassifier() [0.5425     0.5425     0.5275     0.60902256]\nKNeighborsClassifier() [0.4525     0.465      0.4        0.42105263]\nGaussianNB() [0.54       0.5125     0.545      0.60401003]\nLogisticRegression() [0.515      0.5175     0.5875     0.55137845]\nLinearSVC() [0.445      0.4575     0.41       0.53132832]\nMLPClassifier(max_iter=300, random_state=1) [0.5075     0.535      0.5775     0.51378446]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#pramater tuning\nfrom sklearn.model_selection import RandomizedSearchCV , GridSearchCV\n\nn_estimators  = [int(x) for x in range(10,250,50)]\nmax_features =['auto','sqrt', 'log2'] \nmax_depth = [int(x) for x in range(400,650,50)]\nmin_sample_split = [3,4,5,7]\nmin_samples_leaf=[1,2,4]\ncriterion= ['entropy' , 'gini']\n\n\nrandom_grid={\n    'n_estimators' : n_estimators ,\n    'max_features' : max_features ,\n    'max_depth' : max_depth ,\n    'min_samples_split' : min_sample_split ,\n    'min_samples_leaf' : min_samples_leaf ,\n    'criterion':criterion \n}\nrandom_grid\n","execution_count":120,"outputs":[{"output_type":"execute_result","execution_count":120,"data":{"text/plain":"{'n_estimators': [10, 60, 110, 160, 210],\n 'max_features': ['auto', 'sqrt', 'log2'],\n 'max_depth': [400, 450, 500, 550, 600],\n 'min_samples_split': [3, 4, 5, 7],\n 'min_samples_leaf': [1, 2, 4],\n 'criterion': ['entropy', 'gini']}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_random_cv = RandomizedSearchCV(estimator=RandomForestClassifier() ,   param_distributions=random_grid , n_iter=10 ,\n                                cv =4 , verbose=2, random_state=100 ,n_jobs=-1)\n\nmodel_random_cv.fit(X_train, Y_train)\n# print(model_random_cv.best_params_)\n# print(model_random_cv.best_score_)\nprint('cv = ' , 4 , get_score(model_random_cv.best_estimator_ , X_train, X_test, Y_train, Y_test  ))\n","execution_count":121,"outputs":[{"output_type":"stream","text":"Fitting 4 folds for each of 10 candidates, totalling 40 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:    7.6s\n[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:    8.8s finished\n","name":"stderr"},{"output_type":"stream","text":"cv =  4 0.725\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_grid = {\n    'criterion':[model_random_cv.best_params_['criterion']] , \n    'max_depth':[model_random_cv.best_params_['max_depth'] ], \n    'max_features':[model_random_cv.best_params_['max_features']] , \n    'min_samples_leaf':[model_random_cv.best_params_['min_samples_leaf'] ,\n                       model_random_cv.best_params_['min_samples_leaf']+2,\n                       model_random_cv.best_params_['min_samples_leaf'] +4]  , \n    'min_samples_split':[model_random_cv.best_params_['min_samples_split']-2 ,\n                       model_random_cv.best_params_['min_samples_split']-1,\n                       model_random_cv.best_params_['min_samples_split']  ,\n                        model_random_cv.best_params_['min_samples_split'] +1,\n                        model_random_cv.best_params_['min_samples_split'] +2]  ,\n    'n_estimators':[\n                       model_random_cv.best_params_['n_estimators']-100,\n                       model_random_cv.best_params_['n_estimators']  ,\n                        model_random_cv.best_params_['n_estimators'] +100\n                    ]\n\n}\nparam_grid\n\n","execution_count":130,"outputs":[{"output_type":"execute_result","execution_count":130,"data":{"text/plain":"{'criterion': ['gini'],\n 'max_depth': [600],\n 'max_features': ['log2'],\n 'min_samples_leaf': [2, 4, 6],\n 'min_samples_split': [1, 2, 3, 4, 5],\n 'n_estimators': [10, 110, 210]}"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_grid_cv = GridSearchCV(estimator = RandomForestClassifier() , param_grid = param_grid  ,cv =4 ,verbose  =2 , n_jobs=-1)\n\nmodel_grid_cv.fit(X_train, Y_train)\nprint(model_grid_cv.best_params_)\nprint(model_grid_cv.best_score_)\nprint(model_grid_cv.best_estimator_)","execution_count":132,"outputs":[{"output_type":"stream","text":"Fitting 4 folds for each of 45 candidates, totalling 180 fits\n","name":"stdout"},{"output_type":"stream","text":"[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  51 tasks      | elapsed:    7.0s\n[Parallel(n_jobs=-1)]: Done 180 out of 180 | elapsed:   23.5s finished\n","name":"stderr"},{"output_type":"stream","text":"{'criterion': 'gini', 'max_depth': 600, 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 3, 'n_estimators': 210}\n0.6599015478056427\nRandomForestClassifier(max_depth=600, max_features='log2', min_samples_leaf=2,\n                       min_samples_split=3, n_estimators=210)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(get_score(model_grid_cv.best_estimator_ ,X_train, X_test, Y_train, Y_test  ))","execution_count":141,"outputs":[{"output_type":"stream","text":"0.7375\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#cros validation \n#k fold\nfrom sklearn.model_selection import KFold  ,StratifiedKFold\n\nkf = KFold(n_splits=3)\nfor i in model:\n    print(i ,end=' =====    ')\n    \n    for train_index , test_index  in  kf.split(X):\n\n    #     print(train_index.shape)\n    #     print(test_index.shape)\n        print(get_score(i , X.loc[train_index]   , X.loc[test_index] , Y.loc[train_index] , Y.loc[test_index]), end=' ,  ')\n    print()","execution_count":164,"outputs":[{"output_type":"stream","text":"DecisionTreeClassifier() =====    0.5272045028142589 ,  0.450281425891182 ,  0.4821763602251407 ,  \nRandomForestClassifier() =====    0.5328330206378987 ,  0.5834896810506567 ,  0.5853658536585366 ,  \nKNeighborsClassifier() =====    0.47654784240150094 ,  0.44090056285178236 ,  0.4352720450281426 ,  \nGaussianNB() =====    0.5403377110694184 ,  0.5328330206378987 ,  0.525328330206379 ,  \nLogisticRegression() =====    0.5422138836772983 ,  0.5365853658536586 ,  0.5666041275797373 ,  \nLinearSVC() =====    0.09568480300187618 ,  0.5666041275797373 ,  0.49530956848030017 ,  \nMLPClassifier(max_iter=300, random_state=1) =====    0.5159474671669794 ,  0.5272045028142589 ,  0.5309568480300187 ,  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"kf = StratifiedKFold(n_splits=3)\nfor i in model:\n    print(i ,end=' =====    ')\n    \n    for train_index , test_index  in  kf.split(X , Y):\n\n    #     print(train_index.shape)\n    #     print(test_index.shape)\n        print(get_score(i , X.loc[train_index]   , X.loc[test_index] , Y.loc[train_index] , Y.loc[test_index]), end=' ,  ')\n    print()","execution_count":167,"outputs":[{"output_type":"stream","text":"DecisionTreeClassifier() =====    0.4446529080675422 ,  0.4652908067542214 ,  0.4896810506566604 ,  \nRandomForestClassifier() =====    0.5328330206378987 ,  0.6041275797373359 ,  0.5984990619136961 ,  \nKNeighborsClassifier() =====    0.4596622889305816 ,  0.45778611632270166 ,  0.43151969981238275 ,  \nGaussianNB() =====    0.5384615384615384 ,  0.5440900562851783 ,  0.5834896810506567 ,  \nLogisticRegression() =====    0.5215759849906192 ,  0.5684803001876173 ,  0.5403377110694184 ,  \nLinearSVC() =====    0.4521575984990619 ,  0.50093808630394 ,  0.474671669793621 ,  \nMLPClassifier(max_iter=300, random_state=1) =====    0.46904315196998125 ,  0.5272045028142589 ,  0.5215759849906192 ,  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#hence proved random fprest classifier is the best\n\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}